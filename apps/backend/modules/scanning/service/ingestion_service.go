package service

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"log"
	"strings"
	"time"

	lineagesvc "github.com/arc-platform/backend/modules/lineage/service"
	"github.com/arc-platform/backend/modules/shared/domain/entity"
	"github.com/arc-platform/backend/modules/shared/domain/repository"
	"github.com/arc-platform/backend/modules/shared/infrastructure/persistence"
	"github.com/arc-platform/backend/pkg/normalization"
	"github.com/google/uuid"
)

// IngestionService handles scan ingestion and normalization
type IngestionService struct {
	repo            *persistence.PostgresRepository
	classifier      *ClassificationService
	enrichment      *EnrichmentService
	semanticLineage *lineagesvc.SemanticLineageService
}

// NewIngestionService creates a new ingestion service
func NewIngestionService(repo *persistence.PostgresRepository, classifier *ClassificationService, enrichment *EnrichmentService, semanticLineage *lineagesvc.SemanticLineageService) *IngestionService {
	return &IngestionService{
		repo:            repo,
		classifier:      classifier,
		enrichment:      enrichment,
		semanticLineage: semanticLineage,
	}
}

// HawkeyeScanInput represents the Hawk-eye scanner JSON format
type HawkeyeScanInput struct {
	FS         []HawkeyeFinding `json:"fs"`
	PostgreSQL []HawkeyeFinding `json:"postgresql"`
}

// HawkeyeFinding represents a single finding from Hawk-eye
type HawkeyeFinding struct {
	Host                string                 `json:"host"`
	FilePath            string                 `json:"file_path"`
	PatternName         string                 `json:"pattern_name"`
	Matches             []string               `json:"matches"`
	SampleText          string                 `json:"sample_text"`
	Profile             string                 `json:"profile"`
	DataSource          string                 `json:"data_source"`
	FileData            map[string]interface{} `json:"file_data"`
	Severity            string                 `json:"severity"`
	SeverityDescription string                 `json:"severity_description"`
}

// IngestScanResult represents the result of ingestion
type IngestScanResult struct {
	ScanRunID     uuid.UUID `json:"scan_run_id"`
	TotalFindings int       `json:"total_findings"`
	TotalAssets   int       `json:"total_assets"`
	AssetsCreated int       `json:"assets_created"`
	PatternsFound int       `json:"patterns_found"`
}

// IngestScan processes Hawk-eye scan output and normalizes it into the database
func (s *IngestionService) IngestScan(ctx context.Context, input *HawkeyeScanInput) (*IngestScanResult, error) {
	if len(input.FS) == 0 && len(input.PostgreSQL) == 0 {
		return nil, fmt.Errorf("no findings in scan input")
	}

	// BEGIN TRANSACTION - Critical fix for ING-001
	tx, err := s.repo.BeginTx(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to begin transaction: %w", err)
	}

	// Ensure rollback on panic or error
	defer func() {
		if r := recover(); r != nil {
			tx.Rollback()
			log.Printf("PANIC during ingestion, transaction rolled back: %v", r)
			panic(r)
		}
	}()

	// Combine findings
	allFindings := append(input.FS, input.PostgreSQL...)

	// Create ScanRun
	profileName := allFindings[0].Profile
	if profileName == "" {
		profileName = "default"
	}

	scanRun := &entity.ScanRun{
		ID:              uuid.New(),
		ProfileName:     profileName,
		ScanStartedAt:   time.Now().Add(-5 * time.Minute), // Approximate
		ScanCompletedAt: time.Now(),
		Host:            allFindings[0].Host,
		Status:          "completed",
		Metadata:        map[string]interface{}{},
	}

	if err := tx.CreateScanRun(ctx, scanRun); err != nil {
		tx.Rollback()
		return nil, fmt.Errorf("failed to create scan run: %w", err)
	}

	// Track created assets and patterns
	assetMap := make(map[string]uuid.UUID)   // stableID -> UUID
	patternMap := make(map[string]uuid.UUID) // pattern name -> UUID
	assetsCreated := 0

	// Process each finding
	for _, hawkeyeFinding := range allFindings {
		// Generate stable asset ID based on data source type
		// For databases: use table name to create separate assets per table
		// For files: use file path as before
		var assetIdentifier string
		if hawkeyeFinding.DataSource == "postgresql" || hawkeyeFinding.DataSource == "mysql" {
			// Extract table name from path format: "connection string > schema.table.column"
			tableName := extractTableName(hawkeyeFinding.FilePath)
			assetIdentifier = fmt.Sprintf("%s::%s::%s",
				hawkeyeFinding.DataSource,
				hawkeyeFinding.Host,
				tableName)
		} else {
			// For filesystem sources, use file path
			assetIdentifier = hawkeyeFinding.FilePath
		}
		stableID := generateStableID(assetIdentifier)

		// Check if asset already exists
		assetID, exists := assetMap[stableID]
		if !exists {
			// Try to get existing asset from database (using transaction)
			existingAsset, err := tx.GetAssetByStableID(ctx, stableID)
			if err != nil {
				tx.Rollback()
				return nil, fmt.Errorf("failed to check existing asset: %w", err)
			}

			if existingAsset != nil {
				assetID = existingAsset.ID
				assetMap[stableID] = assetID
			} else {
				// Extract owner from file data if available
				owner := "Platform Team"
				if val, ok := hawkeyeFinding.FileData["owner"].(string); ok {
					owner = val
				}

				// Map profile to environment
				env := "Production"
				if scanRun.ProfileName == "test_scan" || scanRun.ProfileName == "dev" {
					env = "Development"
				}

				// Create new asset
				asset := &entity.Asset{
					ID:           uuid.New(),
					StableID:     stableID,
					AssetType:    "file",
					Name:         getFileName(hawkeyeFinding.FilePath),
					Path:         hawkeyeFinding.FilePath,
					DataSource:   hawkeyeFinding.DataSource,
					Host:         hawkeyeFinding.Host,
					Environment:  env,
					Owner:        owner,
					SourceSystem: fmt.Sprintf("%s://%s", hawkeyeFinding.DataSource, hawkeyeFinding.Host),
					FileMetadata: hawkeyeFinding.FileData,
					RiskScore:    calculateRiskScore(hawkeyeFinding.Severity),
				}

				if err := tx.CreateAsset(ctx, asset); err != nil {
					tx.Rollback()
					return nil, fmt.Errorf("failed to create asset: %w", err)
				}

				assetID = asset.ID
				assetMap[stableID] = assetID
				assetsCreated++
			}
		}

		// Get or create pattern
		patternID, err := s.getOrCreatePattern(ctx, &hawkeyeFinding, patternMap)
		if err != nil {
			return nil, fmt.Errorf("failed to get/create pattern: %w", err)
		}

		// ENRICHMENT LAYER - Add contextual intelligence
		// Extract column name if this is a database finding
		columnName := ""
		if colVal, ok := hawkeyeFinding.FileData["column_name"]; ok {
			if colStr, ok := colVal.(string); ok {
				columnName = colStr
			}
		}

		matchSample := ""
		if len(hawkeyeFinding.Matches) > 0 {
			matchSample = hawkeyeFinding.Matches[0]
		}

		// CRITICAL FIX #3: Normalize before classification
		normalizedMatch := normalization.Normalize(matchSample)

		// Perform enrichment
		enrichmentSignals := s.enrichment.Enrich(ctx, EnrichmentContext{
			FilePath:    hawkeyeFinding.FilePath,
			MatchValue:  normalizedMatch, // Use normalized value
			PatternName: hawkeyeFinding.PatternName,
			AssetType:   "file",
			ColumnName:  columnName,
		})

		// Calculate enrichment score (this becomes the Context Score in multi-signal)
		enrichmentScore := s.enrichment.GetEnrichmentScore(enrichmentSignals)

		// Classify finding using multi-signal engine
		multiSignalInput := MultiSignalInput{
			PatternName:       hawkeyeFinding.PatternName,
			FilePath:          hawkeyeFinding.FilePath,
			MatchValue:        normalizedMatch,
			ColumnName:        columnName,
			FileData:          hawkeyeFinding.FileData,
			EnrichmentScore:   enrichmentScore,
			EnrichmentSignals: enrichmentSignals,
		}

		decision, err := s.classifier.ClassifyMultiSignal(ctx, multiSignalInput)
		if err != nil {
			log.Printf("ERROR: Classification failed for %s: %v", hawkeyeFinding.PatternName, err)
			continue
		}

		// RECOMMENDED: Filter Non-PII at ingestion time (60-80% DB size reduction)
		// Alternative: Store all, filter at query time (allows threshold tuning)
		// Current: Using ingestion-time filtering for production efficiency
		// TEMPORARILY DISABLED: Allow all findings through for dashboard visibility
		/*
			if decision.Classification == "Non-PII" || decision.FinalScore < 0.45 {
				// Skip low-confidence and Non-PII findings to prevent database bloat
				// If you need retrospective tuning, comment this block and ensure
				// query-time filtering is applied in finding_repository.go
				continue
			}
		*/

		// Sanitize inputs for Postgres (remove null bytes) with logging
		sanitizedMatches := make([]string, len(hawkeyeFinding.Matches))
		sanitizationCount := 0
		for i, m := range hawkeyeFinding.Matches {
			if strings.Contains(m, "\u0000") {
				sanitizationCount++
				log.Printf("WARNING: Null byte detected in finding %s at %s (removed)",
					hawkeyeFinding.PatternName, hawkeyeFinding.FilePath)
			}
			sanitizedMatches[i] = strings.ReplaceAll(m, "\u0000", "")
		}
		sanitizedSample := strings.ReplaceAll(hawkeyeFinding.SampleText, "\u0000", "")

		// Track sanitization in scan metadata
		if sanitizationCount > 0 {
			if scanRun.Metadata == nil {
				scanRun.Metadata = make(map[string]interface{})
			}
			if existingCount, ok := scanRun.Metadata["sanitized_findings"].(int); ok {
				scanRun.Metadata["sanitized_findings"] = existingCount + sanitizationCount
			} else {
				scanRun.Metadata["sanitized_findings"] = sanitizationCount
			}
		}

		// Convert enrichment signals to map for storage
		enrichmentMap := map[string]interface{}{
			"asset_semantics":   enrichmentSignals.AssetSemantics,
			"environment":       enrichmentSignals.Environment,
			"entropy":           enrichmentSignals.Entropy,
			"charset_diversity": enrichmentSignals.CharsetDiversity,
			"token_shape":       enrichmentSignals.TokenShape,
			"value_hash":        enrichmentSignals.ValueHash,
			"historical_count":  enrichmentSignals.HistoricalCount,
		}

		// Generate normalized hash for deduplication
		// Use pkg/normalization when available, inline implementation for now
		normalizedValue := strings.ToLower(strings.ReplaceAll(strings.ReplaceAll(matchSample, " ", ""), "-", ""))
		hash := sha256.Sum256([]byte(normalizedValue))
		valueHash := hex.EncodeToString(hash[:])
		_ = valueHash // Will be used when entity.Finding has NormalizedValueHash field

		// Check for duplicates (same asset, pattern, and value hash in this scan)
		// Note: This requires adding GetFindingByHash to repository interface
		// For now, we'll just add the hash and rely on the unique index to prevent duplication
		// The database migration 000003_add_deduplication.up.sql adds:
		// CREATE UNIQUE INDEX idx_findings_unique ON findings(asset_id, pattern_name, normalized_value_hash, scan_run_id)

		// Calculate dynamic severity based on classification, confidence, and context
		dynamicSeverity := calculateDynamicSeverity(
			decision.Classification,
			decision.ConfidenceLevel,
			hawkeyeFinding.FileData,
		)

		// Calculate risk score for prioritization (0-100)
		riskScore := calculateComprehensiveRiskScore(
			decision.Classification,
			decision.ConfidenceLevel,
			hawkeyeFinding.FileData,
		)

		// Create finding with deduplication hash
		finding := &entity.Finding{
			ID:                  uuid.New(),
			ScanRunID:           scanRun.ID,
			AssetID:             assetID,
			PatternID:           &patternID,
			PatternName:         hawkeyeFinding.PatternName,
			Matches:             sanitizedMatches,
			SampleText:          sanitizedSample,
			Severity:            dynamicSeverity, // Now calculated from classification+confidence+context
			SeverityDescription: fmt.Sprintf("Risk Score: %d/100 | %s", riskScore, decision.Justification),
			ConfidenceScore:     &decision.FinalScore,
			Context:             decision.SignalBreakdown,
			EnrichmentSignals:   enrichmentMap,
			EnrichmentScore:     &enrichmentScore,
			EnrichmentFailed:    enrichmentSignals.EnrichmentFailed,
			// New field for deduplication:
			// NormalizedValueHash: valueHash,  // Uncomment when entity.Finding has this field
		}

		// TODO: When entity.Finding has NormalizedValueHash field, uncomment above
		// For now, the unique index will prevent true duplicates at DB level

		if err := tx.CreateFinding(ctx, finding); err != nil {
			// Check if error is due to duplicate (unique constraint violation)
			if strings.Contains(err.Error(), "idx_findings_unique") {
				// Duplicate detected - skip silently or log
				log.Printf("DEBUG: Duplicate finding skipped for %s at %s", hawkeyeFinding.PatternName, hawkeyeFinding.FilePath)
				continue
			}
			tx.Rollback()
			return nil, fmt.Errorf("failed to create finding: %w", err)
		}

		// Save Classification
		classification := &entity.Classification{
			ID:                 uuid.New(),
			FindingID:          finding.ID,
			ClassificationType: decision.Classification,
			SubCategory:        decision.SubCategory,
			ConfidenceScore:    decision.FinalScore,
			Justification:      decision.Justification,
			DPDPACategory:      decision.DPDPACategory,
			RequiresConsent:    decision.RequiresConsent,
		}

		if err := tx.CreateClassification(ctx, classification); err != nil {
			tx.Rollback()
			return nil, fmt.Errorf("failed to create classification: %w", err)
		}

		// Create review state
		reviewState := &entity.ReviewState{
			ID:        uuid.New(),
			FindingID: finding.ID,
			Status:    "pending",
		}

		if err := tx.CreateReviewState(ctx, reviewState); err != nil {
			tx.Rollback()
			return nil, fmt.Errorf("failed to create review state: %w", err)
		}
	}

	// Update asset total findings and create relationships
	for stableID, assetID := range assetMap {
		// Count findings for this asset
		count, err := s.repo.CountFindings(ctx, repository.FindingFilters{
			AssetID: &assetID,
		})

		if err != nil {
			return nil, fmt.Errorf("failed to count findings: %w", err)
		}

		// Update asset with finding count
		asset, _ := s.repo.GetAssetByStableID(ctx, stableID)
		if asset != nil {
			asset.TotalFindings = count
			// Recalculate robust risk score based on all findings
			if err := s.recalculateAssetRisk(ctx, assetID); err != nil {
				// Log error but verify other assets
				fmt.Printf("Error recalculating risk for asset %s: %v\n", stableID, err)
			}

			// Sync to Neo4j if semantic lineage is enabled
			if s.semanticLineage != nil {
				log.Printf("INFO: Syncing asset %s to Neo4j semantic graph", assetID)
				if err := s.semanticLineage.SyncAssetToNeo4j(ctx, assetID); err != nil {
					// Log but don't fail ingestion (graceful degradation)
					log.Printf("WARNING: Failed to sync asset %s to Neo4j: %v", assetID, err)
				} else {
					log.Printf("SUCCESS: Asset %s synced to Neo4j", assetID)
				}
			} else {
				log.Printf("INFO: Neo4j semantic lineage is disabled - skipping sync for asset %s", assetID)
			}
		}
	}

	// Update scan run totals
	scanRun.TotalFindings = len(allFindings)
	scanRun.TotalAssets = len(assetMap)
	if err := tx.UpdateScanRun(ctx, scanRun); err != nil {
		tx.Rollback()
		return nil, fmt.Errorf("failed to update scan run: %w", err)
	}

	// CRITICAL FIX: Commit the transaction to persist all changes
	if err := tx.Commit(); err != nil {
		return nil, fmt.Errorf("failed to commit transaction: %w", err)
	}

	return &IngestScanResult{
		ScanRunID:     scanRun.ID,
		TotalFindings: scanRun.TotalFindings,
		TotalAssets:   scanRun.TotalAssets,
		AssetsCreated: assetsCreated,
		PatternsFound: len(patternMap),
	}, nil
}

// recalculateAssetRisk derives the risk score from findings severity and count
func (s *IngestionService) recalculateAssetRisk(ctx context.Context, assetID uuid.UUID) error {
	// 1. Get total findings count
	// We could use CountFindings, but we need max severity too.
	// Let's rely on the repository to give us stats or query findings.

	// For now, simpler: Get ALL findings for this asset (lightweight if paginated/limited, but potentially heavy)
	// BETTER: Add a method to repo: GetAssetRiskData(assetID) -> (count, maxSeverity)
	// Since I can't easily modify the repo interface without touching multiple files,
	// I will use ListFindings logic with a limit, or just count.

	// Actually, I can use CountFindings for count.
	count, err := s.repo.CountFindings(ctx, repository.FindingFilters{
		AssetID: &assetID,
	})
	if err != nil {
		return err
	}

	// 2. Determine Max Severity
	// We verify if there are ANY 'Critical' or 'High' findings.
	hasCritical, err := s.hasFindingWithSeverity(ctx, assetID, "Critical") // "Highest" mapped to Critical in DB?
	// Wait, internal severity is strings: "Highest", "High", "Medium", "Low".
	// The scanner sends "Highest" or "High".
	// Let's check "Highest" (Critical)
	if err != nil {
		return err
	}

	hasHigh, err := s.hasFindingWithSeverity(ctx, assetID, "High")
	if err != nil {
		return err
	}

	// 3. Calculate Base Score
	baseScore := 10
	if hasCritical {
		baseScore = 95
	} else if hasHigh {
		if count > 3 {
			baseScore = 85 // High volume of High severity
		} else {
			baseScore = 75
		}
	} else if count > 0 {
		// Medium/Low
		if count > 10 {
			baseScore = 60
		} else {
			baseScore = 40
		}
	}

	// 4. Update Asset
	return s.repo.UpdateAssetStats(ctx, assetID, baseScore, count)
}

func (s *IngestionService) hasFindingWithSeverity(ctx context.Context, assetID uuid.UUID, severity string) (bool, error) {
	// Quick check using CountFindings filtering
	// Note: Scanner sends "Highest" for Critical. Repo stores what scanner sends (string).
	// My previous fix used "Highest" -> Critical mapping in calculateRiskScore but persisted the raw string.
	// Let's check strict_rules.yml or system.py.
	// verification_output.json showed: "severity": "Highest"

	targetSev := severity
	if severity == "Critical" {
		targetSev = "Highest" // Map back to scanner term if needed, or check both
	}

	count, err := s.repo.CountFindings(ctx, repository.FindingFilters{
		AssetID:  &assetID,
		Severity: targetSev,
	})

	if count > 0 {
		return true, nil
	}

	// Double check alternative naming
	if severity == "Critical" && targetSev == "Highest" {
		// Also check "Critical" just in case
		c2, err := s.repo.CountFindings(ctx, repository.FindingFilters{
			AssetID:  &assetID,
			Severity: "Critical",
		})
		return c2 > 0, err
	}

	return false, err
}

// getOrCreatePattern gets existing pattern or creates new one
func (s *IngestionService) getOrCreatePattern(ctx context.Context, finding *HawkeyeFinding, patternMap map[string]uuid.UUID) (uuid.UUID, error) {
	// Check cache
	if id, exists := patternMap[finding.PatternName]; exists {
		return id, nil
	}

	// Check database
	existingPattern, err := s.repo.GetPatternByName(ctx, finding.PatternName)
	if err != nil {
		return uuid.Nil, err
	}

	if existingPattern != nil {
		patternMap[finding.PatternName] = existingPattern.ID
		return existingPattern.ID, nil
	}

	// Create new pattern
	pattern := &entity.Pattern{
		ID:                uuid.New(),
		Name:              finding.PatternName,
		PatternType:       "regex",
		Category:          categorizePattern(finding.PatternName),
		Description:       fmt.Sprintf("Pattern for detecting: %s", finding.PatternName),
		PatternDefinition: "",
		IsActive:          true,
	}

	if err := s.repo.CreatePattern(ctx, pattern); err != nil {
		return uuid.Nil, err
	}

	patternMap[finding.PatternName] = pattern.ID
	return pattern.ID, nil
}

// extractTableName extracts table name from database finding path
// Path format: "connection string > schema.table.column" or "connection string > table.column"
func extractTableName(filePath string) string {
	// Split by '>' to separate connection from table path
	parts := strings.Split(filePath, ">")
	if len(parts) < 2 {
		return filePath
	}

	// Get the table part and trim whitespace
	tablePart := strings.TrimSpace(parts[1])

	// Split by '.' to get schema.table.column
	dotParts := strings.Split(tablePart, ".")

	if len(dotParts) >= 2 {
		// Return schema.table (ignore column)
		return fmt.Sprintf("%s.%s", dotParts[0], dotParts[1])
	}

	// Fallback to full table part if format is unexpected
	return tablePart
}

// generateStableID creates a stable identifier from asset identifier
func generateStableID(identifier string) string {
	// FIX ING-003: Normalize to prevent duplicates on case-insensitive systems
	// Convert to lowercase to prevent duplicates on macOS/Windows
	normalizedPath := strings.ToLower(identifier)
	hash := sha256.Sum256([]byte(normalizedPath))
	return hex.EncodeToString(hash[:])
}

// getFileName extracts filename from path
func getFileName(path string) string {
	// Simple extraction - in production use filepath.Base
	for i := len(path) - 1; i >= 0; i-- {
		if path[i] == '/' {
			return path[i+1:]
		}
	}
	return path
}

// calculateRiskScore converts severity to numeric risk score
func calculateRiskScore(severity string) int {
	switch severity {
	case "Critical":
		return 95
	case "High":
		return 80
	case "Medium":
		return 60
	case "Low":
		return 30
	default:
		return 10
	}
}

// contains checks if string contains any of the substrings
func contains(str string, substrings []string) bool {
	for _, substr := range substrings {
		if strings.Contains(str, substr) {
			return true
		}
	}
	return false
}

// categorizePattern determines pattern category
func categorizePattern(patternName string) string {
	lowerName := strings.ToLower(patternName)

	if contains(lowerName, []string{"email", "phone", "ssn", "passport", "license", "aadhaar", "aadhar", "pan"}) {
		return "pii"
	}
	if contains(lowerName, []string{"key", "token", "secret", "password", "api", "aws"}) {
		return "secret"
	}
	if contains(lowerName, []string{"credit", "card", "account", "bank"}) {
		return "financial"
	}

	return "other"
}

// Ensure interface compatibility
var _ json.Marshaler = (*HawkeyeScanInput)(nil)

func (h *HawkeyeScanInput) MarshalJSON() ([]byte, error) {
	type Alias HawkeyeScanInput
	return json.Marshal(&struct{ *Alias }{Alias: (*Alias)(h)})
}

// GetLatestScan returns the most recent scan run
func (s *IngestionService) GetLatestScan(ctx context.Context) (*entity.ScanRun, error) {
	return s.repo.GetLatestScanRun(ctx)
}

// calculateDynamicSeverity determines severity based on classification, confidence, and environment
// This creates coherence between severity, classification, and confidence for better interpretability
func calculateDynamicSeverity(classification, confidence string, fileData map[string]interface{}) string {
	// Determine if this is production environment
	isProduction := isProductionEnvironment(fileData)

	// Apply decision matrix: Classification + Confidence + Context = Severity
	switch classification {
	case "Sensitive Personal Data":
		// SSN, PAN, Aadhaar, Credit Cards, etc.
		if confidence == "CONFIRMED" && isProduction {
			return "Critical"
		}
		if confidence == "CONFIRMED" || (confidence == "HIGH_CONFIDENCE" && isProduction) {
			return "High"
		}
		if isProduction {
			return "High"
		}
		return "Medium"

	case "Personal Data":
		// Email, Phone, etc.
		if confidence == "CONFIRMED" && isProduction {
			return "Medium"
		}
		if isProduction {
			return "Low"
		}
		return "Low"

	case "Secrets":
		// API Keys, AWS Keys, etc.
		if confidence == "CONFIRMED" && isProduction {
			return "Critical"
		}
		if isProduction {
			return "High"
		}
		return "Medium"

	default:
		// Non-PII or unknown
		return "Info"
	}
}

// isProductionEnvironment determines if data is from production environment
func isProductionEnvironment(fileData map[string]interface{}) bool {
	if fileData == nil {
		return true // Default to production if unknown (safer)
	}

	// Check environment field
	if env, ok := fileData["environment"].(string); ok {
		envLower := strings.ToLower(env)
		// Non-production indicators
		if strings.Contains(envLower, "test") ||
			strings.Contains(envLower, "dev") ||
			strings.Contains(envLower, "staging") ||
			strings.Contains(envLower, "qa") ||
			strings.Contains(envLower, "sandbox") {
			return false
		}
	}

	// Check database/schema names for test indicators
	if dbName, ok := fileData["database"].(string); ok {
		dbLower := strings.ToLower(dbName)
		if strings.Contains(dbLower, "test") || strings.Contains(dbLower, "dev") {
			return false
		}
	}

	// Default to production
	return true
}

// calculateComprehensiveRiskScore provides numeric risk score (0-100) for sorting and prioritization
// Combines classification sensitivity, confidence level, and environment context
func calculateComprehensiveRiskScore(classification, confidence string, fileData map[string]interface{}) int {
	// Base weights for classification types
	var classificationWeight float64
	switch classification {
	case "Sensitive Personal Data":
		classificationWeight = 100.0
	case "Secrets":
		classificationWeight = 90.0
	case "Personal Data":
		classificationWeight = 50.0
	default:
		classificationWeight = 10.0
	}

	// Confidence multiplier
	var confidenceMultiplier float64
	switch confidence {
	case "CONFIRMED":
		confidenceMultiplier = 1.0
	case "HIGH_CONFIDENCE":
		confidenceMultiplier = 0.75
	case "VALIDATED":
		confidenceMultiplier = 0.5
	default:
		confidenceMultiplier = 0.3
	}

	// Environment context multiplier
	contextMultiplier := 1.0
	if !isProductionEnvironment(fileData) {
		contextMultiplier = 0.3 // Test/dev data is 70% less critical
	}

	// Calculate weighted score
	// Formula: (ClassWeight * 0.6) + (Confidence * 20) + (Context * 20)
	// This ensures classification type dominates, but confidence/context can adjust prioritization

	baseScore := classificationWeight * 0.6
	confidenceScore := (confidenceMultiplier * 100) * 0.2
	contextScore := (contextMultiplier * 100) * 0.2

	totalScore := int(baseScore + confidenceScore + contextScore)

	// Ensure bounds 0-100
	if totalScore > 100 {
		return 100
	}
	if totalScore < 0 {
		return 0
	}

	return totalScore
}

// ClearAllScanData deletes all previous scan data for clean scan-replace workflow
func (s *IngestionService) ClearAllScanData(ctx context.Context) error {
	log.Println("Clearing all previous scan data...")
	_, err := s.repo.GetDB().ExecContext(ctx, `
		TRUNCATE findings, assets, classifications, 
		asset_relationships, review_states, scan_runs, finding_feedback 
		CASCADE
	`)
	if err != nil {
		return fmt.Errorf("failed to clear scan data: %w", err)
	}
	log.Println("âœ… All previous scan data cleared successfully")
	return nil
}
